{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d31dd171-fd74-4c12-b24a-9bf5eef2377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from re import search\n",
    "import json\n",
    "from time import sleep\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7780f3-f2b8-4991-b96e-5c6b636e2352",
   "metadata": {},
   "source": [
    "# Парсинг ссылок\n",
    "Вдохновившись новостями по нобелевской неделе, для парсинга я выбрал сайт Naked Science, который содержит научнопопулярные статьи и новости из мира науки. Из раздела со всеми новостями будем парсить по страницам название каждой статьи, автора, аннотацию, индекс важности (оценка автором важности исследования/открытия для науки), дату публикации, хэштеги, количество просмотров в качестве целевой переменной, и наконец ссылки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95fa7f14-e58e-4d7c-9f93-1f3b2ba8c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Так как дата публикации и число просмотров имеет разные форматы, сделаем функцию для их извлечения\n",
    "\n",
    "def parse_date(date, form='%d.%m.%Y'):\n",
    "    mapper = {\n",
    "        'января': '01',\n",
    "        'февраля': '02',\n",
    "        'марта': '03',\n",
    "        'апреля': '04',\n",
    "        'мая': '05',\n",
    "        'июня': '06',\n",
    "        'июля': '07',\n",
    "        'августа': '08',\n",
    "        'сентября': '09',\n",
    "        'октября': '10',\n",
    "        'ноября': '11',\n",
    "        'декабря': '12'\n",
    "    }\n",
    "    if search(r'\\d{1,2}\\.\\d{2}\\.\\d{4}', date):\n",
    "        return date\n",
    "    else:\n",
    "        date = date.split()\n",
    "        day = '0' + date[0] if len(date[0]) < 2 else date[0]\n",
    "        return '.'.join((day, mapper[date[1]], '2025'))\n",
    "\n",
    "def parse_views(v):\n",
    "    if v[-3:] == 'тыс':\n",
    "        num = v.split()[0]\n",
    "        num = num.replace(',', '.')\n",
    "        v = float(num) * (10 ** 3)\n",
    "    \n",
    "    return int(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0748981a-c70b-4141-af21-e70878805959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(obj):\n",
    "    header = obj.find('a', attrs={'class': 'animate-custom'})\n",
    "    link = header.attrs['href'].strip()\n",
    "    \n",
    "    title = header.get_text().split()\n",
    "    imp_ind = float(title[-1])\n",
    "    title = ' '.join(title[:-1])\n",
    "\n",
    "    author = obj.find(lambda tag: tag.name == 'div' and tag.get('class') == ['meta-item', 'meta-item_author'])\n",
    "    author = author.string.strip()\n",
    "\n",
    "    date = obj.find('span', attrs={'class': 'echo_date'}).string\n",
    "    date = parse_date(date.split(', ')[0])\n",
    "\n",
    "    views = obj.find('span', attrs={'class': 'fvc-count'}).string\n",
    "    views = parse_views(views)\n",
    "\n",
    "    annot = obj.find('p').string.strip()\n",
    "\n",
    "    tags = obj.find(lambda tag: tag.name == 'div' and tag.get('class') == ['terms-items', 'grid'])\n",
    "    tags = tags.find_all('div', attrs={'class': 'terms-item'})\n",
    "    tags = [t.get_text().replace('#', '').strip() for t in tags]\n",
    "\n",
    "    return {\n",
    "        'title': title,\n",
    "        'author': author,\n",
    "        'date': date,\n",
    "        'imp_ind': imp_ind,\n",
    "        'views': views,\n",
    "        'annot': annot,\n",
    "        'tags': tags,\n",
    "        'link': link\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a60595b-bf5f-4cac-bca5-37beb6558647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing page 5/500\n",
      "processing page 10/500\n",
      "processing page 15/500\n",
      "processing page 20/500\n",
      "processing page 25/500\n",
      "processing page 30/500\n",
      "processing page 35/500\n",
      "processing page 40/500\n",
      "processing page 45/500\n",
      "processing page 50/500\n",
      "processing page 55/500\n",
      "processing page 60/500\n",
      "processing page 65/500\n",
      "processing page 70/500\n",
      "processing page 75/500\n",
      "processing page 80/500\n",
      "processing page 85/500\n",
      "processing page 90/500\n",
      "processing page 95/500\n",
      "processing page 100/500\n",
      "processing page 105/500\n",
      "processing page 110/500\n",
      "processing page 115/500\n",
      "processing page 120/500\n",
      "processing page 125/500\n",
      "processing page 130/500\n",
      "processing page 135/500\n",
      "processing page 140/500\n",
      "processing page 145/500\n",
      "processing page 150/500\n",
      "processing page 155/500\n",
      "processing page 160/500\n",
      "processing page 165/500\n",
      "processing page 170/500\n",
      "processing page 175/500\n",
      "processing page 180/500\n",
      "processing page 185/500\n",
      "processing page 190/500\n",
      "processing page 195/500\n",
      "processing page 200/500\n",
      "processing page 205/500\n",
      "processing page 210/500\n",
      "processing page 215/500\n",
      "processing page 220/500\n",
      "processing page 225/500\n",
      "processing page 230/500\n",
      "processing page 235/500\n",
      "processing page 240/500\n",
      "processing page 245/500\n",
      "processing page 250/500\n",
      "processing page 255/500\n",
      "processing page 260/500\n",
      "processing page 265/500\n",
      "processing page 270/500\n",
      "processing page 275/500\n",
      "processing page 280/500\n",
      "processing page 285/500\n",
      "processing page 290/500\n",
      "processing page 295/500\n",
      "processing page 300/500\n",
      "processing page 305/500\n",
      "processing page 310/500\n",
      "processing page 315/500\n",
      "processing page 320/500\n",
      "processing page 325/500\n",
      "processing page 330/500\n",
      "processing page 335/500\n",
      "processing page 340/500\n",
      "processing page 345/500\n",
      "processing page 350/500\n",
      "processing page 355/500\n",
      "processing page 360/500\n",
      "processing page 365/500\n",
      "processing page 370/500\n",
      "processing page 375/500\n",
      "processing page 380/500\n",
      "processing page 385/500\n",
      "processing page 390/500\n",
      "processing page 395/500\n",
      "processing page 400/500\n",
      "processing page 405/500\n",
      "processing page 410/500\n",
      "processing page 415/500\n",
      "processing page 420/500\n",
      "processing page 425/500\n",
      "processing page 430/500\n",
      "processing page 435/500\n",
      "processing page 440/500\n",
      "processing page 445/500\n",
      "processing page 450/500\n",
      "processing page 455/500\n",
      "processing page 460/500\n",
      "processing page 465/500\n",
      "processing page 470/500\n",
      "processing page 475/500\n",
      "processing page 480/500\n",
      "processing page 485/500\n",
      "processing page 490/500\n",
      "processing page 495/500\n",
      "\n",
      " --- 500/500 pages have been successfully parsed ---\n"
     ]
    }
   ],
   "source": [
    "user_ag = UserAgent()\n",
    "main_link = 'https://naked-science.ru/article/page/'\n",
    "attemps = 0\n",
    "success = []\n",
    "\n",
    "start, pages = 10, 500\n",
    "articles, links = [], []\n",
    "\n",
    "for i in range(start, pages + start): # начнем пасрить с 10 страницы, чтобы количество просмотров и коммментов уже немного установилось\n",
    "    if i % 5 == 0 and i != start:\n",
    "        print(f'processing page {i - start}/{pages}')\n",
    "        with open('articles_links.json', 'w', encoding='UTF-8') as f:\n",
    "            json.dump(articles, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    for j in range(3): # Пытаемся спарсить страницу три раза, если не удалось, увеличиваем счеткчик неудачных попыток\n",
    "        sleep(uniform(1.1, 1.9))\n",
    "        response = requests.get(main_link + f'{i}/', headers={'User-Agent': user_ag.random})\n",
    "        if not response.ok:\n",
    "            print(f'\\n  WARNING: page {i} parsing failed\\n')\n",
    "            continue\n",
    "        else:\n",
    "            success.append(i)\n",
    "            attemps = 0\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            news = soup.find('div', attrs={'class': 'news-items'})\n",
    "            news = news.find_all(lambda tag: tag.name == 'div' and tag.get('class') == ['news-item-left', 'with-bookmark'])\n",
    "            for x in news:\n",
    "                try:\n",
    "                    a = get_info(x)\n",
    "                    if a['link'] not in links: # Некоторые статьи могут парситься дважды, поэтому проверяем\n",
    "                        articles.append(a)\n",
    "                        links.append(a['link'])\n",
    "                except:\n",
    "                    pass\n",
    "            break\n",
    "    else:\n",
    "        attemps += 1\n",
    "\n",
    "    if attemps > 3:\n",
    "        # Если не удалось спарсить больше трех страниц подряд, будем прерывать процесс\n",
    "        print(f'\\n  WARNING: Parsing was stopped\\n')\n",
    "        break\n",
    "\n",
    "\n",
    "print(f'\\n --- {len(success)}/{pages} pages have been successfully parsed ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2612c803-7b7e-48af-b845-5500f9c09584",
   "metadata": {},
   "source": [
    "# Парсинг статей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5b9a5a0-0a3e-465d-995c-dad5c69549d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8272"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('articles_links.json', 'r', encoding='UTF-8') as f:\n",
    "    articles = json.load(f)\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df0166ed-2acd-47cc-8231-e933caf9eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page(responce, par_sep='\\n'):\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    obj = soup.find('div', attrs={'class': 'body'}).find_all('p')\n",
    "    text = par_sep.join([par.get_text().strip() for par in obj])\n",
    "\n",
    "    comments = soup.find('div', attrs={'class': 'shesht-comments-list'})\n",
    "    comments = comments.find_all('div', attrs={'class': 'shesht-comment-template__content-text'})\n",
    "    comments = [co.get_text().strip() for co in comments]\n",
    "\n",
    "    return {'text': text, 'comments': comments}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4787d38-5330-4b22-a192-da652f9fcab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ag = UserAgent()\n",
    "attemps = 0\n",
    "success = []\n",
    "\n",
    "all_pages = len(articles)\n",
    "pages, links = [], [x['link'] for x in articles]\n",
    "\n",
    "for i, link in enumerate(links):\n",
    "    if i % 5 == 0:\n",
    "        print(f'processing page {i}/{all_pages}')\n",
    "        with open('texts_comments.json', 'w', encoding='UTF-8') as f:\n",
    "            json.dump(pages, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    for j in range(3): # Пытаемся спарсить страницу три раза, если не удалось, увеличиваем счеткчик неудачных попыток\n",
    "        sleep(uniform(1.1, 1.9))\n",
    "        response = requests.get(link, headers={'User-Agent': user_ag.random})\n",
    "        if not response.ok:\n",
    "            print(f'\\n  WARNING: page {i} parsing failed\\n')\n",
    "            continue\n",
    "        else:\n",
    "            success.append(i)\n",
    "            attemps = 0\n",
    "            try:\n",
    "                a = parse_page(response)\n",
    "                a['link'] = link\n",
    "                pages.append(a)\n",
    "            except:\n",
    "                pass\n",
    "            break\n",
    "    else:\n",
    "        attemps += 1\n",
    "\n",
    "    if attemps > 3:\n",
    "        # Если не удалось спарсить больше трех страниц подряд, будем прерывать процесс\n",
    "        print(f'\\n  WARNING: Parsing was stopped\\n')\n",
    "        break\n",
    "\n",
    "\n",
    "print(f'\\n --- {len(success)}/{pages} pages have been successfully parsed ---')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:my-env]",
   "language": "python",
   "name": "conda-env-my-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
