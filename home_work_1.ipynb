{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d31dd171-fd74-4c12-b24a-9bf5eef2377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from re import search\n",
    "import json\n",
    "from time import sleep\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7780f3-f2b8-4991-b96e-5c6b636e2352",
   "metadata": {},
   "source": [
    "# Парсинг ссылок\n",
    "Вдохновившись новостями по нобелевской неделе, для парсинга я выбрал сайт Naked Science, который содержит научнопопулярные статьи и новости из мира науки. Из раздела со всеми новостями будем парсить по страницам название каждой статьи, автора, аннотацию, индекс важности (оценка автором важности исследования/открытия для науки), дату публикации, хэштеги, количество просмотров в качестве целевой переменной, и наконец ссылки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95fa7f14-e58e-4d7c-9f93-1f3b2ba8c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Так как дата публикации имеет разный формат для текущего года и предыдущих, сделаем функцию для выделения даты\n",
    "\n",
    "def parse_date(date, form='%d.%m.%Y'):\n",
    "    mapper = {\n",
    "        'января': '01',\n",
    "        'февраля': '02',\n",
    "        'марта': '03',\n",
    "        'апреля': '04',\n",
    "        'мая': '05',\n",
    "        'июня': '06',\n",
    "        'июля': '07',\n",
    "        'августа': '08',\n",
    "        'сентября': '09',\n",
    "        'октября': '10',\n",
    "        'ноября': '11',\n",
    "        'декабря': '12'\n",
    "    }\n",
    "    if search(r'\\d{1,2}\\.\\d{2}\\.\\d{4}', date):\n",
    "        return date\n",
    "    else:\n",
    "        date = date.split()\n",
    "        day = '0' + date[0] if len(date[0]) < 2 else date[0]\n",
    "        return '.'.join((day, mapper[date[1]], '2025'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0748981a-c70b-4141-af21-e70878805959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(obj):\n",
    "    header = obj.find('a', attrs={'class': 'animate-custom'})\n",
    "    link = header.attrs['href'].strip()\n",
    "    \n",
    "    title = header.get_text().split()\n",
    "    imp_ind = float(title[-1])\n",
    "    title = ' '.join(title[:-1])\n",
    "\n",
    "    author = obj.find(lambda tag: tag.name == 'div' and tag.get('class') == ['meta-item', 'meta-item_author'])\n",
    "    author = author.string.strip()\n",
    "\n",
    "    date = obj.find('span', attrs={'class': 'echo_date'}).string\n",
    "    date = parse_date(date.split(', ')[0])\n",
    "\n",
    "    views = obj.find('span', attrs={'class': 'fvc-count'}).string\n",
    "    views = int(views.replace(' ', ''))\n",
    "\n",
    "    annot = obj.find('p').string.strip()\n",
    "\n",
    "    tags = obj.find(lambda tag: tag.name == 'div' and tag.get('class') == ['terms-items', 'grid'])\n",
    "    tags = tags.find_all('div', attrs={'class': 'terms-item'})\n",
    "    tags = [t.get_text().replace('#', '').strip() for t in tags]\n",
    "\n",
    "    return {\n",
    "        'title': title,\n",
    "        'author': author,\n",
    "        'date': date,\n",
    "        'imp_ind': imp_ind,\n",
    "        'views': views,\n",
    "        'annot': annot,\n",
    "        'tags': tags,\n",
    "        'link': link\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a60595b-bf5f-4cac-bca5-37beb6558647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing page 5/400\n",
      "processing page 10/400\n",
      "processing page 15/400\n",
      "processing page 20/400\n",
      "processing page 25/400\n",
      "processing page 30/400\n",
      "processing page 35/400\n",
      "processing page 40/400\n",
      "processing page 45/400\n",
      "processing page 50/400\n",
      "processing page 55/400\n",
      "processing page 60/400\n",
      "processing page 65/400\n",
      "processing page 70/400\n",
      "processing page 75/400\n",
      "processing page 80/400\n",
      "processing page 85/400\n",
      "processing page 90/400\n",
      "processing page 95/400\n",
      "processing page 100/400\n",
      "processing page 105/400\n",
      "processing page 110/400\n",
      "processing page 115/400\n",
      "processing page 120/400\n",
      "processing page 125/400\n",
      "processing page 130/400\n",
      "processing page 135/400\n",
      "processing page 140/400\n",
      "processing page 145/400\n",
      "processing page 150/400\n",
      "processing page 155/400\n",
      "processing page 160/400\n",
      "processing page 165/400\n",
      "processing page 170/400\n",
      "processing page 175/400\n",
      "processing page 180/400\n",
      "processing page 185/400\n",
      "processing page 190/400\n",
      "processing page 195/400\n",
      "processing page 200/400\n",
      "processing page 205/400\n",
      "processing page 210/400\n",
      "processing page 215/400\n",
      "processing page 220/400\n",
      "processing page 225/400\n",
      "processing page 230/400\n",
      "processing page 235/400\n",
      "processing page 240/400\n",
      "processing page 245/400\n",
      "processing page 250/400\n",
      "processing page 255/400\n",
      "processing page 260/400\n",
      "processing page 265/400\n",
      "processing page 270/400\n",
      "processing page 275/400\n",
      "processing page 280/400\n",
      "processing page 285/400\n",
      "processing page 290/400\n",
      "processing page 295/400\n",
      "processing page 300/400\n",
      "processing page 305/400\n",
      "processing page 310/400\n",
      "processing page 315/400\n",
      "processing page 320/400\n",
      "processing page 325/400\n",
      "processing page 330/400\n",
      "processing page 335/400\n",
      "processing page 340/400\n",
      "processing page 345/400\n",
      "processing page 350/400\n",
      "processing page 355/400\n",
      "processing page 360/400\n",
      "processing page 365/400\n",
      "processing page 370/400\n",
      "processing page 375/400\n",
      "processing page 380/400\n",
      "processing page 385/400\n",
      "processing page 390/400\n",
      "processing page 395/400\n",
      "processing page 400/400\n",
      "\n",
      " --- 400/400 pages have been successfully parsed ---\n"
     ]
    }
   ],
   "source": [
    "user_ag = UserAgent()\n",
    "main_link = 'https://naked-science.ru/article/page/'\n",
    "attemps = 0\n",
    "success = []\n",
    "\n",
    "pages = 400\n",
    "articles, links = [], []\n",
    "\n",
    "for i in range(1, pages + 1):\n",
    "    if i % 5 == 0:\n",
    "        print(f'processing page {i}/{pages}')\n",
    "        with open('articles_links.json', 'w', encoding='UTF-8') as f:\n",
    "            json.dump(articles, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    for j in range(3): # Пытаемся спарсить страницу три раза, если не удалось, увеличиваем счеткчик неудачных попыток\n",
    "        sleep(uniform(1.1, 1.9))\n",
    "        response = requests.get(main_link + f'{i}/', headers={'User-Agent': user_ag.random})\n",
    "        if not response.ok:\n",
    "            print(f'\\n  WARNING: page {i} parsing failed\\n')\n",
    "            continue\n",
    "        else:\n",
    "            success.append(i)\n",
    "            attemps = 0\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            news = soup.find('div', attrs={'class': 'news-items'})\n",
    "            news = news.find_all(lambda tag: tag.name == 'div' and tag.get('class') == ['news-item-left', 'with-bookmark'])\n",
    "            for x in news:\n",
    "                try:\n",
    "                    a = get_info(x)\n",
    "                    if a['link'] not in links: # Некоторые статьи могут парситься дважды, поэтому проверяем\n",
    "                        articles.append(a)\n",
    "                        links.append(a['link'])\n",
    "                except:\n",
    "                    pass\n",
    "            break\n",
    "    else:\n",
    "        attemps += 1\n",
    "\n",
    "    if attemps > 3:\n",
    "        # Если не удалось спарсить больше трех страниц подряд, будем прерывать процесс\n",
    "        print(f'\\n  WARNING: Parsing was stopped\\n')\n",
    "        break\n",
    "\n",
    "\n",
    "print(f'\\n --- {len(success)}/{pages} pages have been successfully parsed ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2612c803-7b7e-48af-b845-5500f9c09584",
   "metadata": {},
   "source": [
    "# Парсинг статей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5b9a5a0-0a3e-465d-995c-dad5c69549d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7041"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('articles_links.json', 'r', encoding='UTF-8') as f:\n",
    "    articles = json.load(f)\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ebcefe5b-ed49-49c7-bd89-1557cabbf7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ag = UserAgent()\n",
    "main_link = articles[89]['link']\n",
    "response = requests.get(main_link, headers={'User-Agent': user_ag.random})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df0166ed-2acd-47cc-8231-e933caf9eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(responce, par_sep='\\n'):\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    obj = soup.find('div', attrs={'class': 'body'}).find_all('p')\n",
    "\n",
    "    return par_sep.join([par.get_text().strip() for par in obj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a4787d38-5330-4b22-a192-da652f9fcab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = parse_text(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7b701d4d-1323-4b1f-8ba5-a9b3fbd3111a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8840"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.getsizeof(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39aa37bf-badb-4ffa-ab74-d6a47426171a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://naked-science.ru/article/astronomy/reagirovaniya-na-signaly'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_link"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:my-env]",
   "language": "python",
   "name": "conda-env-my-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
