{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d31dd171-fd74-4c12-b24a-9bf5eef2377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from re import search\n",
    "import json\n",
    "from time import sleep\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7780f3-f2b8-4991-b96e-5c6b636e2352",
   "metadata": {},
   "source": [
    "# Парсинг \n",
    "Вдохновившись новостями по нобелевской неделе, для парсинга я выбрал сайт Naked Science, который содержит научнопопулярные статьи и новости из мира науки. Из каждой статьи можно вытащить ее название, автора, аннотацию, индекс важности (оценка автором важности исследования/открытия для науки), дату публикации, сам текст статьи, хэштеги и количество просмотров в качестве целевой переменной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95fa7f14-e58e-4d7c-9f93-1f3b2ba8c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Так как дата публикации имеет разный формат для текущего года и предыдущих, сделаем функцию для выделения даты\n",
    "\n",
    "def parse_date(date, form='%d.%m.%Y'):\n",
    "    mapper = {\n",
    "        'января': '01',\n",
    "        'февраля': '02',\n",
    "        'марта': '03',\n",
    "        'апреля': '04',\n",
    "        'мая': '05',\n",
    "        'июня': '06',\n",
    "        'июля': '07',\n",
    "        'августа': '08',\n",
    "        'сентября': '09',\n",
    "        'октября': '10',\n",
    "        'ноября': '11',\n",
    "        'декабря': '12'\n",
    "    }\n",
    "    if search(r'\\d{1,2}\\.\\d{2}\\.\\d{4}', date):\n",
    "        return date\n",
    "    else:\n",
    "        date = date.split()\n",
    "        day = '0' + date[0] if len(date[0]) < 2 else date[0]\n",
    "        return '.'.join((day, mapper[date[1]], '2025'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0748981a-c70b-4141-af21-e70878805959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(obj):\n",
    "    header = obj.find('a', attrs={'class': 'animate-custom'})\n",
    "    link = header.attrs['href'].strip()\n",
    "    \n",
    "    title = header.get_text().split()\n",
    "    imp_ind = float(title[-1])\n",
    "    title = ' '.join(title[:-1])\n",
    "\n",
    "    author = obj.find(lambda tag: tag.name == 'div' and tag.get('class') == ['meta-item', 'meta-item_author'])\n",
    "    author = author.string.strip()\n",
    "\n",
    "    date = obj.find('span', attrs={'class': 'echo_date'}).string\n",
    "    date = parse_date(date.split(', ')[0])\n",
    "\n",
    "    views = obj.find('span', attrs={'class': 'fvc-count'}).string\n",
    "    views = int(views.replace(' ', ''))\n",
    "\n",
    "    annot = obj.find('p').string.strip()\n",
    "\n",
    "    tags = obj.find(lambda tag: tag.name == 'div' and tag.get('class') == ['terms-items', 'grid'])\n",
    "    tags = tags.find_all('div', attrs={'class': 'terms-item'})\n",
    "    tags = [t.get_text().replace('#', '').strip().lower() for t in tags]\n",
    "\n",
    "    return {\n",
    "        'title': title,\n",
    "        'author': author,\n",
    "        'date': date,\n",
    "        'imp_ind': imp_ind,\n",
    "        'views': views,\n",
    "        'annot': annot,\n",
    "        'tags': tags,\n",
    "        'link': link\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a60595b-bf5f-4cac-bca5-37beb6558647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing page 5/10\n",
      "processing page 10/10\n",
      "\n",
      " --- 10/10 pages have been successfully parsed ---\n"
     ]
    }
   ],
   "source": [
    "user_ag = UserAgent()\n",
    "main_link = 'https://naked-science.ru/article/page/'\n",
    "attemps = 0\n",
    "success = []\n",
    "\n",
    "pages = 10\n",
    "articles, links = [], []\n",
    "\n",
    "for i in range(1, pages + 1):\n",
    "    if i % 5 == 0:\n",
    "        print(f'processing page {i}/{pages}')\n",
    "        with open('articles_links.json', 'w', encoding='UTF-8') as f:\n",
    "            json.dump(articles, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    for j in range(3): # Пытаемся спарсить страницу три раза, если не удалось, увеличиваем счеткчик неудачных попыток\n",
    "        sleep(uniform(1.1, 1.9))\n",
    "        response = requests.get(main_link + f'{i}/', headers={'User-Agent': user_ag.random})\n",
    "        if not response.ok:\n",
    "            print(f'\\n  WARNING: page {i} parsing failed\\n')\n",
    "            continue\n",
    "        else:\n",
    "            success.append(i)\n",
    "            attemps = 0\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            news = soup.find('div', attrs={'class': 'news-items'})\n",
    "            news = news.find_all(lambda tag: tag.name == 'div' and tag.get('class') == ['news-item-left', 'with-bookmark'])\n",
    "            for x in news:\n",
    "                try:\n",
    "                    a = get_info(x)\n",
    "                    if a['link'] not in links: # Некоторые статьи могут парситься дважды, поэтому проверяем\n",
    "                        articles.append(a)\n",
    "                        links.append(a['link'])\n",
    "                except:\n",
    "                    pass\n",
    "            break\n",
    "    else:\n",
    "        attemps += 1\n",
    "\n",
    "    if attemps > 3:\n",
    "        # Если не удалось спарсить больше трех страниц подряд, будем прерывать процесс\n",
    "        print(f'\\n  WARNING: Parsing was stopped, {i - 1} pages were processed')\n",
    "        break\n",
    "\n",
    "\n",
    "print(f'\\n --- {len(success)}/{pages} pages have been successfully parsed ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b185d2dd-b44b-4928-82cc-81ab22ecbc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:my-env]",
   "language": "python",
   "name": "conda-env-my-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
